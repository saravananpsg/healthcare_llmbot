{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "96c1693b0e42495ca853af3a0d4c0815": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_43542631950640c7ba88db69132e4572",
              "IPY_MODEL_dd2cfb7bf370449b86d0f003bf5865d2",
              "IPY_MODEL_48aa2f7cbba74d64968c8e2101b6c087"
            ],
            "layout": "IPY_MODEL_df50935a05ed44d1badc51a4f0cb8709"
          }
        },
        "43542631950640c7ba88db69132e4572": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2179ef09f3b844fd83e44125434a5425",
            "placeholder": "​",
            "style": "IPY_MODEL_a5d2b5cd12834fddb90ee8fd92ef734c",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "dd2cfb7bf370449b86d0f003bf5865d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_20e22beed88441569c963ab833415a65",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c9d4b87980ba44f3aad1bc630fa619c0",
            "value": 3
          }
        },
        "48aa2f7cbba74d64968c8e2101b6c087": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6879b86489144f41b337530ae938684e",
            "placeholder": "​",
            "style": "IPY_MODEL_c5e92be4dea94a41a3e49cf6638fd270",
            "value": " 3/3 [00:04&lt;00:00,  1.36s/it]"
          }
        },
        "df50935a05ed44d1badc51a4f0cb8709": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2179ef09f3b844fd83e44125434a5425": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5d2b5cd12834fddb90ee8fd92ef734c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "20e22beed88441569c963ab833415a65": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9d4b87980ba44f3aad1bc630fa619c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6879b86489144f41b337530ae938684e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5e92be4dea94a41a3e49cf6638fd270": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "maockUevlqFR",
        "outputId": "1505412e-aee1-44c0-c29d-99ecf2f19aef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SelfCheck-MQAG initialized to device cuda\n",
            "SelfCheck-BERTScore initialized\n",
            "SelfCheck-1gram initialized\n",
            "['Michael Alan Weiner (born March 31, 1942) is an American radio host.', 'He is the host of The Savage Nation.']\n",
            "SelfCheck-MQAG: [0.21206286 0.4177945 ]\n",
            "SelfCheck-BERTScore: [0.05884961 0.53198777]\n",
            "SelfCheck-Ngram scores: {'sent_level': {'avg_neg_logprob': [3.184312427726156, 3.279774864365169], 'max_neg_logprob': [3.476098689835273, 4.574710978503383]}, 'doc_level': {'avg_neg_logprob': 3.218678904916201, 'avg_max_neg_logprob': 4.025404834169327}}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0.21206286, 0.4177945 ]),\n",
              " array([0.05884961, 0.53198777]),\n",
              " {'sent_level': {'avg_neg_logprob': [3.184312427726156, 3.279774864365169],\n",
              "   'max_neg_logprob': [3.476098689835273, 4.574710978503383]},\n",
              "  'doc_level': {'avg_neg_logprob': 3.218678904916201,\n",
              "   'avg_max_neg_logprob': 4.025404834169327}})"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# !pip install selfcheckgpt transformers spacy\n",
        "\n",
        "import torch\n",
        "from selfcheckgpt.modeling_selfcheck import SelfCheckMQAG, SelfCheckBERTScore, SelfCheckNgram\n",
        "from transformers import pipeline\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def set_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "def self_check_llm(passage, sample_list):\n",
        "  selfcheck_mqag = SelfCheckMQAG(device=device) # set device to 'cuda' if GPU is available\n",
        "  selfcheck_bertscore = SelfCheckBERTScore(rescale_with_baseline=True)\n",
        "  selfcheck_ngram = SelfCheckNgram(n=1) # n=1 means Unigram, n=2 means Bigram, etc.\n",
        "  # --------------------------------------------------------------------------------------------------------------- #\n",
        "  # Split passage into sentences\n",
        "  sentences = [sent.text.strip() for sent in nlp(passage).sents] # spacy sentence tokenization\n",
        "  sample1, sample2, sample3 = sample_list[0], sample_list[1],sample_list[2]\n",
        "  # --------------------------------------------------------------------------------------------------------------- #\n",
        "  # SelfCheck-MQAG: Score for each sentence where value is in [0.0, 1.0] and high value means non-factual\n",
        "  # Additional params for each scoring_method:\n",
        "  # -> counting: AT (answerability threshold, i.e. questions with answerability_score < AT are rejected)\n",
        "  # -> bayes: AT, beta1, beta2\n",
        "  # -> bayes_with_alpha: beta1, beta2\n",
        "  sent_scores_mqag = selfcheck_mqag.predict(\n",
        "      sentences = sentences,               # list of sentences\n",
        "      passage = passage,                   # passage (before sentence-split)\n",
        "      sampled_passages = [sample1, sample2, sample3], # list of sampled passages\n",
        "      num_questions_per_sent = 5,          # number of questions to be drawn\n",
        "      scoring_method = 'bayes_with_alpha', # options = 'counting', 'bayes', 'bayes_with_alpha'\n",
        "      beta1 = 0.8, beta2 = 0.8,            # additional params depending on scoring_method\n",
        "  )\n",
        "  print(\"SelfCheck-MQAG:\", sent_scores_mqag)\n",
        "\n",
        "  # --------------------------------------------------------------------------------------------------------------- #\n",
        "  # SelfCheck-BERTScore: Score for each sentence where value is in [0.0, 1.0] and high value means non-factual\n",
        "  sent_scores_bertscore = selfcheck_bertscore.predict(\n",
        "      sentences = sentences,                          # list of sentences\n",
        "      sampled_passages = [sample1, sample2, sample3], # list of sampled passages\\\n",
        "      )\n",
        "  print(\"SelfCheck-BERTScore:\", sent_scores_bertscore)\n",
        "\n",
        "  # --------------------------------------------------------------------------------------------------------------- #\n",
        "  # SelfCheck-Ngram: Score at sentence- and document-level where value is in [0.0, +inf) and high value means non-factual\n",
        "  # as opposed to SelfCheck-MQAG and SelfCheck-BERTScore, SelfCheck-Ngram's score is not bounded\n",
        "  sent_scores_ngram = selfcheck_ngram.predict(\n",
        "      sentences = sentences,\n",
        "      passage = passage,\n",
        "      sampled_passages = [sample1, sample2, sample3],\n",
        "      )\n",
        "  print(\"SelfCheck-Ngram scores:\",sent_scores_ngram)\n",
        "\n",
        "  return sent_scores_mqag, sent_scores_bertscore, sent_scores_ngram\n",
        "\n",
        "\n",
        "# {'sent_level': { # sentence-level score similar to MQAG and BERTScore variant\n",
        "#     'avg_neg_logprob': [3.184312, 3.279774],\n",
        "#     'max_neg_logprob': [3.476098, 4.574710]\n",
        "#     },\n",
        "#  'doc_level': {  # document-level score such that avg_neg_logprob is computed over all tokens\n",
        "#     'avg_neg_logprob': 3.218678904916201,\n",
        "#     'avg_max_neg_logprob': 4.025404834169327\n",
        "#     }\n",
        "# }\n",
        "\n",
        "set_seed(42)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Other samples generated by the same LLM to perform self-check for consistency\n",
        "sample1 = \"Michael Alan Weiner (born March 31, 1942) is an American radio host. He is the host of The Savage Country.\"\n",
        "sample2 = \"Michael Alan Weiner (born January 13, 1960) is a Canadian radio host. He works at The New York Times.\"\n",
        "sample3 = \"Michael Alan Weiner (born March 31, 1942) is an American radio host. He obtained his PhD from MIT.\"\n",
        "sample_list = [sample1, sample2, sample3]\n",
        "# LLM's text (e.g. GPT-3 response) to be evaluated at the sentence level  & Split it into sentences\n",
        "passage = \"Michael Alan Weiner (born March 31, 1942) is an American radio host. He is the host of The Savage Nation.\"\n",
        "\n",
        "self_check_llm(passage, sample_list)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from selfcheckgpt.modeling_selfcheck import SelfCheckNLI\n",
        "selfcheck_nli = SelfCheckNLI(device=device) # set device to 'cuda' if GPU is available\\\n",
        "sample_list = [sample1, sample2, sample3]\n",
        "sentences = [sent.text.strip() for sent in nlp(passage).sents] # spacy sentence tokenization\n",
        "print(sentences)\n",
        "sent_scores_nli = selfcheck_nli.predict(\n",
        "    sentences = sentences,                          # list of sentences\n",
        "    sampled_passages = sample_list, # list of sampled passages\n",
        ")\n",
        "print(sent_scores_nli)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgWJ4hdXpKfb",
        "outputId": "4b7a67da-65a7-45af-b6e4-925ba55bfb75"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SelfCheck-NLI initialized to device cuda\n",
            "['Michael Alan Weiner (born March 31, 1942) is an American radio host.', 'He is the host of The Savage Nation.']\n",
            "[0.33401404 0.97510576]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Option1: open-source model\n",
        "from selfcheckgpt.modeling_selfcheck import SelfCheckLLMPrompt\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "llm_model = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "selfcheck_prompt = SelfCheckLLMPrompt(llm_model, device)\n",
        "\n",
        "sent_scores_prompt = selfcheck_prompt.predict(\n",
        "    sentences = sentences,                          # list of sentences\n",
        "    sampled_passages = sample_list, # list of sampled passages\n",
        "    verbose = True, # whether to show a progress bar\n",
        ")\n",
        "\n",
        "print(sent_scores_prompt)\n",
        "# [0.33333333, 0.66666667] -- based on the example above\n",
        "\n",
        "\n",
        "# Option2: API access (currently only support client_type=\"openai\")\n",
        "# from selfcheckgpt.modeling_selfcheck_apiprompt import SelfCheckAPIPrompt\n",
        "# selfcheck_prompt = SelfCheckAPIPrompt(client_type=\"openai\", model=\"gpt-3.5-turbo\")\n",
        "\n",
        "# sent_scores_prompt = selfcheck_prompt.predict(\n",
        "#     sentences = sentences,                          # list of sentences\n",
        "#     sampled_passages = [sample1, sample2, sample3], # list of sampled passages\n",
        "#     verbose = True, # whether to show a progress bar\n",
        "# )\n",
        "# print(sent_scores_prompt)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103,
          "referenced_widgets": [
            "96c1693b0e42495ca853af3a0d4c0815",
            "43542631950640c7ba88db69132e4572",
            "dd2cfb7bf370449b86d0f003bf5865d2",
            "48aa2f7cbba74d64968c8e2101b6c087",
            "df50935a05ed44d1badc51a4f0cb8709",
            "2179ef09f3b844fd83e44125434a5425",
            "a5d2b5cd12834fddb90ee8fd92ef734c",
            "20e22beed88441569c963ab833415a65",
            "c9d4b87980ba44f3aad1bc630fa619c0",
            "6879b86489144f41b337530ae938684e",
            "c5e92be4dea94a41a3e49cf6638fd270"
          ]
        },
        "id": "bTGbB22xpU2A",
        "outputId": "f45790c6-7338-4b36-821e-86f7fc959203"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "96c1693b0e42495ca853af3a0d4c0815"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SelfCheck-LLMPrompt (mistralai/Mistral-7B-Instruct-v0.2) initialized to device cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [00:01<00:00,  1.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.33333333 0.66666667]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}