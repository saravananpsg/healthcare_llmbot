# -*- coding: utf-8 -*-
"""
Created from saravananpsg/drive/self-consistency-LLM-Response.ipynb
(Requires: GPU) evaluated in Google colab notebooks
"""
# !pip install selfcheckgpt transformers spacy
import torch
from selfcheckgpt.modeling_selfcheck import SelfCheckMQAG, SelfCheckBERTScore, SelfCheckNgram
from transformers import pipeline
import spacy
from selfcheckgpt.modeling_selfcheck import SelfCheckNLI
from selfcheckgpt.modeling_selfcheck import SelfCheckLLMPrompt
nlp = spacy.load("en_core_web_sm")

def set_seed(seed):
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)


def self_check_llm(passage, sample_list):
  """
  Self-Consistency check for LLM response:
  This is to mitigate the LLM hallucination and to evaluate the LLM Response
  consistency of text generation:
  Metrics Used:
  - SelfCheck-MQAG
  - BERTScore
  - Selfcheck_ngram
  """
  selfcheck_mqag = SelfCheckMQAG(device=device) # set device to 'cuda' if GPU is available
  selfcheck_bertscore = SelfCheckBERTScore(rescale_with_baseline=True)
  selfcheck_ngram = SelfCheckNgram(n=1) # n=1 means Unigram, n=2 means Bigram, etc.
  # --------------------------------------------------------------------------------------------------------------- #
  # Split passage into sentences
  sentences = [sent.text.strip() for sent in nlp(passage).sents] # spacy sentence tokenization
  sample1, sample2 = sample_list[0], sample_list[1]
  # --------------------------------------------------------------------------------------------------------------- #
  # 9 Score for each sentence where value is in [0.0, 1.0] and high value means non-factual
  # Additional params for each scoring_method:
  # -> counting: AT (answerability threshold, i.e. questions with answerability_score < AT are rejected)
  # -> bayes: AT, beta1, beta2
  # -> bayes_with_alpha: beta1, beta2
  sent_scores_mqag = selfcheck_mqag.predict(
      sentences = sentences,               # list of sentences
      passage = passage,                   # passage (before sentence-split)
      sampled_passages = [sample1, sample2], # list of sampled passages
      num_questions_per_sent = 5,          # number of questions to be drawn
      scoring_method = 'bayes_with_alpha', # options = 'counting', 'bayes', 'bayes_with_alpha'
      beta1 = 0.8, beta2 = 0.8,            # additional params depending on scoring_method
  )
  print("SelfCheck-MQAG:", sent_scores_mqag)

  # --------------------------------------------------------------------------------------------------------------- #
  # SelfCheck-BERTScore: Score for each sentence where value is in [0.0, 1.0] and high value means non-factual
  sent_scores_bertscore = selfcheck_bertscore.predict(
      sentences = sentences,                          # list of sentences
      sampled_passages = [sample1, sample2], # list of sampled passages\
      )
  print("SelfCheck-BERTScore:", sent_scores_bertscore)

  # --------------------------------------------------------------------------------------------------------------- #
  # SelfCheck-Ngram: Score at sentence- and document-level where value is in [0.0, +inf) and high value means non-factual
  # as opposed to SelfCheck-MQAG and SelfCheck-BERTScore, SelfCheck-Ngram's score is not bounded
  sent_scores_ngram = selfcheck_ngram.predict(
      sentences = sentences,
      passage = passage,
      sampled_passages = [sample1, sample2],
      )
  print("SelfCheck-Ngram scores:", sent_scores_ngram)
  return sent_scores_mqag, sent_scores_bertscore, sent_scores_ngram

if __name__ == "__main__":

    set_seed(42)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Other samples generated by the same LLM to perform self-check for consistency
    sample1 = "Gestational diabetes is a type of diabetes that develops during pregnancy. It is characterized by high blood sugar levels that can pose risks to both the mother and the baby. Gestational diabetes is usually diagnosed through screening tests during pregnancy. According to the articles provided, the diagnosis of gestational diabetes involves screening all women at 24 to 28 weeks of gestation using a 3-point 75 g oral glucose tolerance test (OGTT). Additionally, women at increased risk of pre-existing diabetes are recommended to be screened for diabetes during their first trimester using non-pregnancy glucose thresholds. If the results are normal, these women should be re-evaluated for gestational diabetes at 24 to 28 weeks of gestation."
    sample2 = "Gestational diabetes is a type of diabetes that develops during pregnancy and usually goes away after delivery. It is diagnosed using a 3-point 75 g oral glucose tolerance test (OGTT) at 24 to 28 weeks of gestation, unless the woman has already been diagnosed with diabetes or pre-diabetes. It is important to also screen for pre-existing diabetes in the first trimester and after delivery, as women with a history of GDM are at increased risk of developing type 2 diabetes later on in life."
    sample_list = [sample1, sample2]

    # LLM's text (e.g. GPT-3 response) to be evaluated at the sentence level  & Split it into sentences
    passage = "Gestational diabetes is a type of diabetes that occurs during pregnancy. It is diagnosed through a screening process that involves testing a pregnant woman's blood sugar levels. This usually occurs during the second trimester, around 24 to 28 weeks of pregnancy. The screening test is called a 3-point 75 g oral glucose tolerance test (OGTT), which involves drinking a sugary drink and then having blood drawn at three different time points to measure how the body responds to the sugar. If a woman is found to have high blood sugar levels during this test, she is diagnosed with gestational diabetes. It is important to diagnose and manage gestational diabetes to prevent complications for both the mother and baby."
    sentences = [sent.text.strip() for sent in nlp(passage).sents] # spacy sentence tokenization
    print(sentences)

    # self-consistency check for the LLM response
    self_check_llm(passage, sample_list)


    # Option1: open-source model
    llm_model = "mistralai/Mistral-7B-Instruct-v0.2"
    selfcheck_prompt = SelfCheckLLMPrompt(llm_model, device)

    sent_scores_prompt = selfcheck_prompt.predict(
        sentences = sentences,                          # list of sentences
        sampled_passages = sample_list, # list of sampled passages
        verbose = True, # whether to show a progress bar
    )
    print(sent_scores_prompt)

    # Option2: NLI based
    selfcheck_nli = SelfCheckNLI(device=device)  # set device to 'cuda' if GPU is available\

    sent_scores_nli = selfcheck_nli.predict(
        sentences=sentences,  # list of sentences
        sampled_passages=sample_list,  # list of sampled passages
    )
    print(sent_scores_nli)

    # Option3: API access (currently only support client_type="openai")
    # from selfcheckgpt.modeling_selfcheck_apiprompt import SelfCheckAPIPrompt
    # selfcheck_prompt = SelfCheckAPIPrompt(client_type="openai", model="gpt-3.5-turbo")

    # sent_scores_prompt = selfcheck_prompt.predict(
    #     sentences = sentences,                          # list of sentences
    #     sampled_passages = [sample1, sample2, sample3], # list of sampled passages
    #     verbose = True, # whether to show a progress bar
    # )
    # print(sent_scores_prompt)
